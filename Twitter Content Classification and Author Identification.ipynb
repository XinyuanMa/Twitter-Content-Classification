{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Content Classification and Author Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input shape is:  (4743, 2)\n",
      "test_input shape is:  (1701, 2)\n"
     ]
    }
   ],
   "source": [
    "train_input = pd.read_csv('train.csv')\n",
    "test_input = pd.read_csv('test.csv')\n",
    "print \"train_input shape is: \", np.shape(train_input)\n",
    "print \"test_input shape is: \", np.shape(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_tweet(input_set):\n",
    "    handle = input_set['handle']\n",
    "    # put it into an array named label, \n",
    "    # where 0 represents HillaryClinton, \n",
    "    # 1 represents readDonaldTrump\n",
    "    label = []\n",
    "    for i in range(len(handle)):\n",
    "        if handle[i] == \"HillaryClinton\":\n",
    "            label.append(0)\n",
    "        if handle[i] == \"realDonaldTrump\":\n",
    "            label.append(1)\n",
    "    label = np.asarray(label)\n",
    "    return label\n",
    "\n",
    "train_label = label_tweet(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = train_input['tweet'].as_matrix()\n",
    "test_corpus = test_input['tweet'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the method demonstrated here, we also tried another tokenizer called TweetTokenizer in NLTK library, which is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.tokenize.casual import TweetTokenizer\n",
    "# def tokenization(tknzr, tweet):\n",
    "#     tweet = filter(lambda word: not re.match('[https://]', word), tweet)\n",
    "#     tokens = tknzr.tokenize(tweet)\n",
    "#     return tokens   \n",
    "# tknzer = TweetTokenizer()\n",
    "# train_corpus_tokenized = []\n",
    "# for i in range(len(train_corpus)):\n",
    "#     train_corpus_tokenized.append(tokenization(tknzer, train_corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it turned out that this cannot produce neat results.\n",
    "<p>We also tried character-wise tokenization.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def tokenization(text):\n",
    "#     return [i for i in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>[['T', 'h', 'e', 'q', 'u', 'e', 's', 't', 'i', 'o', 'n', 'i', 'n', 't', 'h', 'i', 's', 'e', 'l', 'e', 'c', 't', 'i', 'o', 'n', ':', 'W', 'h', 'o', 'c', 'a', 'n', 'p', 'u', 't', 't', 'h', 'e', 'p', 'l', 'a', 'n', 's', 'i', 'n', 't', 'o', 'a', 'c', 't', 'i', 'o', 'n', 't', 'h', 'a', 't', 'w', 'i', 'l', 'l', 'm', 'a', 'k', 'e', 'y', 'o', 'u', 'r', 'l', 'i', 'f', 'e', 'b', 'e', 't', 't', 'e', 'r', '?', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 'X', 'r', 'e', 'E', 'Y', '9', 'O', 'i', 'c', 'G'], ['L', 'a', 's', 't', 'n', 'i', 'g', 'h', 't', ',', 'D', 'o', 'n', 'a', 'l', 'd', 'T', 'r', 'u', 'm', 'p', 's', 'a', 'i', 'd', 'n', 'o', 't', 'p', 'a', 'y', 'i', 'n', 'g', 't', 'a', 'x', 'e', 's', 'w', 'a', 's', '\"', 's', 'm', 'a', 'r', 't', '.', '\"', 'Y', 'o', 'u', 'k', 'n', 'o', 'w', 'w', 'h', 'a', 't', 'I', 'c', 'a', 'l', 'l', 'i', 't', '?', 'U', 'n', 'p', 'a', 't', 'r', 'i', 'o', 't', 'i', 'c', '.', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 't', '0', 'x', 'm', 'B', 'f', 'j', '7', 'z', 'F'], ['I', 'f', 'w', 'e', 's', 't', 'a', 'n', 'd', 't', 'o', 'g', 'e', 't', 'h', 'e', 'r', ',', 't', 'h', 'e', 'r', 'e', \"'\", 's', 'n', 'o', 't', 'h', 'i', 'n', 'g', 'w', 'e', 'c', 'a', 'n', \"'\", 't', 'd', 'o', '.', 'M', 'a', 'k', 'e', 's', 'u', 'r', 'e', 'y', 'o', 'u', \"'\", 'r', 'e', 'r', 'e', 'a', 'd', 'y', 't', 'o', 'v', 'o', 't', 'e', ':', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 't', 'T', 'g', 'e', 'q', 'x', 'N', 'q', 'Y', 'm', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 'Q', '3', 'Y', 'm', 'b', 'b', '7', 'U', 'N', 'y'], ['B', 'o', 't', 'h', 'c', 'a', 'n', 'd', 'i', 'd', 'a', 't', 'e', 's', 'w', 'e', 'r', 'e', 'a', 's', 'k', 'e', 'd', 'a', 'b', 'o', 'u', 't', 'h', 'o', 'w', 't', 'h', 'e', 'y', \"'\", 'd', 'c', 'o', 'n', 'f', 'r', 'o', 'n', 't', 'r', 'a', 'c', 'i', 'a', 'l', 'i', 'n', 'j', 'u', 's', 't', 'i', 'c', 'e', '.', 'O', 'n', 'l', 'y', 'o', 'n', 'e', 'h', 'a', 'd', 'a', 'r', 'e', 'a', 'l', 'a', 'n', 's', 'w', 'e', 'r', '.', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 's', 'j', 'n', 'E', 'o', 'k', 'c', 'k', 'i', 's'], ['J', 'o', 'i', 'n', 'm', 'e', 'f', 'o', 'r', 'a', '3', 'p', 'm', 'r', 'a', 'l', 'l', 'y', '-', 't', 'o', 'm', 'o', 'r', 'r', 'o', 'w', 'a', 't', 't', 'h', 'e', 'M', 'i', 'd', '-', 'A', 'm', 'e', 'r', 'i', 'c', 'a', 'C', 'e', 'n', 't', 'e', 'r', 'i', 'n', 'C', 'o', 'u', 'n', 'c', 'i', 'l', 'B', 'l', 'u', 'f', 'f', 's', ',', 'I', 'o', 'w', 'a', '!', 'T', 'i', 'c', 'k', 'e', 't', 's', ':', '\\xe2', '\\x80', '\\xa6', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 'd', 'f', 'z', 's', 'b', 'I', 'C', 'i', 'X', 'c']]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the result of this tokenization method and it makes the vectors very large. Finally the accuracy of this model is 90%. It turned out that this method does not necessarily benefit a classification model.\n",
    "<p>Instead a tokenizer is written in a very straightforward way. It throws away common stop words, as well as domain names starting with 'https://', which are all useless in classifing whose tweet is it.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# 'https' seems useless, so I add it to stop_words\n",
    "stop_words.append(u'https')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization, training set and testing set look like:\n",
      "[u'question election put plans action make life better', u'last night donald trump said paying taxes smart know call unpatriotic', u\"stand together 's nothing ca n't make sure 're ready vote\", u\"candidates asked 'd confront racial injustice one real answer\", u'join 3pm rally tomorrow mid-america center council bluffs iowa tickets']\n",
      "[u\"could n't proud hillaryclinton vision command last night 's debate showed 's ready next potus\", u\"election important sit go make sure 're registered nationalvoterregistrationday -h\", u'government people join movement today', u\"national voterregistrationday make sure 're registered vote makeamericagreatagain\\u2026\", u'great afternoon little havana hispanic community leaders thank support imwithyou']\n"
     ]
    }
   ],
   "source": [
    "def tokenization(text):\n",
    "    tokens=[]\n",
    "    for word in nltk.word_tokenize(text.decode('utf-8')):\n",
    "        # skip all the websites, punctuations, pure digits\n",
    "        if not re.match('[//]', word) and re.search('[a-zA-Z]', word) and word.lower() not in stop_words:\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "# Tokenize training set\n",
    "train_corpus_tokenized = []\n",
    "for i in train_corpus:\n",
    "    train_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "\n",
    "# Tokenize testing set\n",
    "test_corpus_tokenized = []\n",
    "for i in test_corpus:\n",
    "    test_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "\n",
    "print \"After tokenization, training set and testing set look like:\"\n",
    "print(train_corpus_tokenized[:5])\n",
    "print(test_corpus_tokenized[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenized tweets look good, but wordwise form is needed so that every word can be counted and stored in a word dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this step is to assign every word an ID so that later we can transform every tweet into a vector of word IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tf.compat.as_str, training set and testing set look like:\n",
      "[['question', 'election', 'put', 'plans', 'action', 'make', 'life', 'better'], ['last', 'night', 'donald', 'trump', 'said', 'paying', 'taxes', 'smart', 'know', 'call', 'unpatriotic'], ['stand', 'together', \"'s\", 'nothing', 'ca', \"n't\", 'make', 'sure', \"'re\", 'ready', 'vote'], ['candidates', 'asked', \"'d\", 'confront', 'racial', 'injustice', 'one', 'real', 'answer'], ['join', '3pm', 'rally', 'tomorrow', 'mid-america', 'center', 'council', 'bluffs', 'iowa', 'tickets']]\n",
      "[['could', \"n't\", 'proud', 'hillaryclinton', 'vision', 'command', 'last', 'night', \"'s\", 'debate', 'showed', \"'s\", 'ready', 'next', 'potus'], ['election', 'important', 'sit', 'go', 'make', 'sure', \"'re\", 'registered', 'nationalvoterregistrationday', '-h'], ['government', 'people', 'join', 'movement', 'today'], ['national', 'voterregistrationday', 'make', 'sure', \"'re\", 'registered', 'vote', 'makeamericagreatagain\\xe2\\x80\\xa6'], ['great', 'afternoon', 'little', 'havana', 'hispanic', 'community', 'leaders', 'thank', 'support', 'imwithyou']]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized_word = []\n",
    "for i in range(len(train_corpus_tokenized)):\n",
    "    train_tokenized_word.append(tf.compat.as_str(train_corpus_tokenized[i]).split())\n",
    "\n",
    "test_tokenized_word = []\n",
    "for i in range(len(test_corpus_tokenized)):\n",
    "    test_tokenized_word.append(tf.compat.as_str(test_corpus_tokenized[i]).split())\n",
    "    \n",
    "print \"After tf.compat.as_str, training set and testing set look like:\"\n",
    "print(train_tokenized_word[:5])\n",
    "print(test_tokenized_word[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build dataset which assigns ids to each word and vectorize tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altogether there are: 8507 words\n"
     ]
    }
   ],
   "source": [
    "cnt = collections.Counter()\n",
    "for i in range(len(train_tokenized_word)):\n",
    "    for word in train_tokenized_word[i]:\n",
    "        cnt[word] += 1\n",
    "\n",
    "print 'Altogether there are: ' + str((len(cnt))) + ' words'\n",
    "\n",
    "vocabulary_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So ID number should range from 1 to 8507, and by default the more a word appears, the smaller its ID should be. Next we should build a function to assign each ID to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we went to https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/tutorials/word2vec/word2vec_basic.py for reference while building this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(cnt, words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(cnt.most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        inner_data = []\n",
    "        for word in words[i]:\n",
    "            index = dictionary.get(word, 0)\n",
    "            if index == 0:  # dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            inner_data.append(index)\n",
    "        data.append(inner_data)\n",
    "        \n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "train_x, count, dictionary, reverse_dictionary = build_dataset(cnt, train_tokenized_word, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data set size is: 1701, such as:\n",
      "[[[92, 6, 123, 56, 1097, 0, 44, 83, 2, 101, 1224, 2, 200, 160, 32], [0, 0]], [[108, 271, 2004, 50, 12, 148, 76, 616, 1765, 81], [0, 0]], [[1276, 9, 43, 212, 27], [0, 0]], [[118, 0, 12, 148, 76, 616, 22, 0], [0, 0]], [[5, 933, 244, 0, 2910, 606, 517, 4, 66, 168], [0, 0]]]\n"
     ]
    }
   ],
   "source": [
    "# Process testing data\n",
    "test = []\n",
    "for sentence in test_tokenized_word:\n",
    "    cur = []\n",
    "    for word in sentence:\n",
    "        if(word in dictionary):\n",
    "            cur.append(dictionary[word])\n",
    "        else:\n",
    "            cur.append(0)\n",
    "    # to store corresponding label\n",
    "    test.append([cur,[0, 0]])\n",
    "\n",
    "test_length = len(test)\n",
    "print \"Testing data set size is: \" + str((len(test))) + \", such as:\"\n",
    "print(test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divided the training dataset into two parts to form an actual training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set size is: 3794, such as:\n",
      "[[[102, 2192, 16, 213, 2371], [0, 1]], [[411, 142, 2093, 300, 1430, 907, 100, 9, 65, 846, 150, 1392, 8219, 1], [0, 1]], [[27, 2, 1593, 56, 271, 1314, 1015, 8268, 224, 1170, 224, 138, 626, 100, 46], [0, 1]], [[59, 7, 1, 69, 2303, 137, 31, 2, 69, 380, 1721, 31], [0, 1]], [[38, 11, 2, 216, 509, 3645, 46, 17, 2958, 955, 1958], [0, 1]]]\n",
      "Validation data set size is: 949, such as:\n",
      "[[[4, 6592, 1626, 187, 181, 197, 1719, 1881, 2, 653], [1, 0]], [[20, 53, 51, 354, 21, 42], [1, 0]], [[1, 2, 3317, 977, 3468, 772, 909, 121, 94, 1198, 7728, 59], [0, 1]], [[4157, 3058, 499, 3008, 431, 7328, 3417, 305, 4273, 7973, 1862], [0, 1]], [[103, 184, 6502, 128, 667], [0, 1]]]\n"
     ]
    }
   ],
   "source": [
    "# Process training data\n",
    "train_all = [[train_x[i], [train_label[i], 1-train_label[i]]] for i in range(0, len(train_x))]\n",
    "\n",
    "# shuffle the training set in which to pick training and validation sets\n",
    "r_index = list(range(len(train_all)))\n",
    "random.shuffle(r_index)\n",
    "train = [train_all[i] for i in r_index[:int(len(r_index)*0.8)]]\n",
    "valid = [train_all[i] for i in r_index[int(len(r_index)*0.8):]]\n",
    "\n",
    "print \"Training data set size is: \" + str((len(train))) + \", such as:\"\n",
    "print(train[:5])\n",
    "print \"Validation data set size is: \" + str((len(valid))) + \", such as:\"\n",
    "print(valid[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tweet has different length. While passing batches of vectorized tweet into the RNN model, their lengths should be identical. Therefore we do the padding for each batch of tweets to make shorter vectors longer by appending 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleDataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.df)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n > self.size:\n",
    "            self.epochs += 1\n",
    "            print(\"SimpleDataIterator epoch : \", self.epochs)\n",
    "            self.shuffle()\n",
    "        res = self.df[self.cursor : self.cursor + n]\n",
    "        self.cursor += n\n",
    "        return res\n",
    "\n",
    "class PaddedDataIterator(SimpleDataIterator):\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df[self.cursor : self.cursor + n]\n",
    "        self.cursor += n\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        max_len = 0\n",
    "        for row in res:\n",
    "            if len(row[0]) > max_len:\n",
    "                max_len = len(row[0])\n",
    "        seqlen = np.array([max_len for i in range(len(res))])\n",
    "        ret = []\n",
    "        label = []\n",
    "        for row in res:\n",
    "            ret += [row[0] + [0] * (max_len - len(row[0]))]\n",
    "            label.append(row[1][0])\n",
    "        x = np.array(ret)\n",
    "        y = np.array(label)\n",
    "\n",
    "        return x, y, seqlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some tests about the classes specified above to make sure they work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences is like this:\n",
      "[[[6, 130, 225, 5, 1009, 39, 1138, 4943, 16, 920], [0, 1]], [[7, 1, 90, 86, 20, 201, 1193, 1063, 13, 863, 49, 26, 20, 201, 87, 62, 6, 560], [0, 1]], [[1449, 8, 38, 292, 8264, 382, 334, 955, 2748, 13, 454, 2841], [1, 0]], [[196, 672, 776, 91, 1115, 661, 637, 431], [0, 1]], [[3968, 8, 916, 725, 194, 1, 9, 5], [1, 0]]]\n"
     ]
    }
   ],
   "source": [
    "data = SimpleDataIterator(valid)\n",
    "d = data.next_batch(500)\n",
    "print 'Input sequences is like this:' \n",
    "print d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences in one random batch is now like:\n",
      "[[   9   45 7453    3  104   19 7306 8189  193    0    0    0]\n",
      " [   3   15  470    7    1    2  219  188 2354 4097 2179 5113]\n",
      " [  60   85  125  110   20 6916   32 1594 1978  221 1943    0]]\n",
      "with shape of: \n",
      "(3, 12)\n",
      "Corresponding labels are: \n",
      "[0 0 0]\n",
      "where 0 stands for Hillary and 1 for Trump.\n"
     ]
    }
   ],
   "source": [
    "data = PaddedDataIterator(train)\n",
    "d = data.next_batch(3)\n",
    "print 'Input sequences in one random batch is now like:'\n",
    "print d[0]\n",
    "print 'with shape of: '\n",
    "print d[0].shape\n",
    "print 'Corresponding labels are: '\n",
    "print d[1]\n",
    "print 'where 0 stands for Hillary and 1 for Trump.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above batch is just an example. Every time we run the code snippet above, it provides a different batch example, but they all show that after padding, each batch of tweets has identical length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the padding for the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set length = 1701\n",
      "batch size = 256\n",
      "so there should be 7 batches\n",
      " \n",
      "testing batch 1 complete, with the vector length equals 19\n",
      "testing batch 2 complete, with the vector length equals 18\n",
      "testing batch 3 complete, with the vector length equals 23\n",
      "testing batch 4 complete, with the vector length equals 20\n",
      "testing batch 5 complete, with the vector length equals 20\n",
      "testing batch 6 complete, with the vector length equals 18\n",
      "testing batch 7 complete, with the vector length equals 19\n"
     ]
    }
   ],
   "source": [
    "def align(data):\n",
    "    max_len = 0\n",
    "    for row in data:\n",
    "        if len(row[0]) > max_len:\n",
    "            max_len = len(row[0])\n",
    "    ret = []\n",
    "    label = []\n",
    "    for row in data:\n",
    "        ret += [row[0] + [0]*(max_len - len(row[0]))]\n",
    "        label.append(row[1][0])\n",
    "    x = np.array(ret)\n",
    "    y = np.array(label)\n",
    "    seq_len = np.array([max_len for i in data])\n",
    "    \n",
    "    return x, y, seq_len\n",
    "\n",
    "batch_size = 256\n",
    "print 'test set length = %d' % test_length\n",
    "print 'batch size = %d' % batch_size\n",
    "print 'so there should be %d batches' % (test_length / batch_size + 1)\n",
    "print ' '\n",
    "\n",
    "test_list = []\n",
    "test_addlen = test\n",
    "test_addlen.extend(test[0:batch_size])\n",
    "for i in range(test_length / batch_size + 1):\n",
    "    x, y, seq_len = align(test[i * batch_size : (i+1) * batch_size])\n",
    "    print 'testing batch ' + str(i + 1) + ' complete, with the vector length equals ' + str(x.shape[1])\n",
    "    test_list.append([x, y, seq_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build RNN model with a GRU cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we build an RNN model. This model contains a GRU cell, which is a good substitution for LSTM.\n",
    "<img src=\"LSTM3-var-GRU.png\">\n",
    "This image is from http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model contains 4 parameters: vocabulary size, hidden state size, batch size and number of classes. 2 of them are hyper parameters that can be tuned. They are hidden state size and batch size.\n",
    "<p>If hidden state size is too small, the result would not be precise enough; if it is too big, it causes overfitting.</p>\n",
    "<p>Batch size decides the running time. If batch size is too small, the running time could be long.</p>\n",
    "<p>Also, we use AdamOptimizer as the optimizer. We have tried GradientDescentOptimizer but the result was far from good. Inside AdamOptimizer, the learning rate could also be tuned. The default is 0.001, which has the result of 2.18 in Kaggle. We changed it to 0.0001 to make a better result.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build RNN model\n",
    "def build_graph(\n",
    "    vocab_size = len(dictionary),\n",
    "    state_size = 60,\n",
    "    batch_size = 256,\n",
    "    num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Obtain the last relevant output\n",
    "    idx = tf.range(batch_size) * tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "\n",
    "    # finally use a Softmax layer to output a probability\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    \n",
    "    # evaluate the model\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "   \n",
    "    # optimizer, which could be tuned\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the RNN model and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the RNN model until the accuracy converges. The number of epochs needed for the accuracy to converge depends on the hyper parameters, such as the batch size, optimizer learning rate and hidden state size. \n",
    "<p>Note that the epoch does not have to be very large. If the training accuracy grows to as much as 0.99, it could be possible that the model overfits the training set, thus fits not that good for the validation set as well as testing set.</p>\n",
    "<p>While the model is being trained, real-time accuracies of training set and validation set are printed.</p>\n",
    "<p>After training, we make predictions of the test based on the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_graph(g, batch_size = 256, num_epochs = 15, iterator = PaddedDataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        tv = iterator(valid)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, tv_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                # evaluate validation set\n",
    "                tv_epoch = tv.epochs\n",
    "                while tv.epochs == tv_epoch:\n",
    "                    step += 1\n",
    "                    batch = tv.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                tv_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print 'Accuracy after epoch %d is: ' % current_epoch \n",
    "                print 'training: %f, validation: %f' % (tr_losses[-1], tv_losses[-1])\n",
    "        print '---------- RNN training done! ----------'\n",
    "            \n",
    "        # make predictions with the current model\n",
    "        predictions = []\n",
    "        for te in test_list:\n",
    "            feed = {g['x']: te[0], g['y']: te[1], g['seqlen']: te[2]}\n",
    "            preds_, _ = sess.run([g['preds'], g['ts']], feed_dict=feed)\n",
    "            predictions.extend(preds_)\n",
    "\n",
    "    return tr_losses, tv_losses, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1 is: \n",
      "training: 0.507031, validation: 0.514648\n",
      "Accuracy after epoch 2 is: \n",
      "training: 0.507812, validation: 0.519531\n",
      "Accuracy after epoch 3 is: \n",
      "training: 0.525112, validation: 0.554688\n",
      "Accuracy after epoch 4 is: \n",
      "training: 0.761998, validation: 0.786458\n",
      "Accuracy after epoch 5 is: \n",
      "training: 0.839844, validation: 0.808594\n",
      "Accuracy after epoch 6 is: \n",
      "training: 0.875558, validation: 0.845052\n",
      "Accuracy after epoch 7 is: \n",
      "training: 0.873047, validation: 0.846354\n",
      "Accuracy after epoch 8 is: \n",
      "training: 0.932199, validation: 0.888021\n",
      "Accuracy after epoch 9 is: \n",
      "training: 0.940011, validation: 0.901042\n",
      "Accuracy after epoch 10 is: \n",
      "training: 0.943080, validation: 0.889323\n",
      "Accuracy after epoch 11 is: \n",
      "training: 0.944196, validation: 0.911458\n",
      "Accuracy after epoch 12 is: \n",
      "training: 0.953125, validation: 0.895833\n",
      "Accuracy after epoch 13 is: \n",
      "training: 0.954520, validation: 0.903646\n",
      "Accuracy after epoch 14 is: \n",
      "training: 0.963170, validation: 0.901042\n",
      "Accuracy after epoch 15 is: \n",
      "training: 0.969308, validation: 0.924479\n",
      "---------- RNN training done! ----------\n"
     ]
    }
   ],
   "source": [
    "g = build_graph()\n",
    "tr_losses, tv_losses, predictions = train_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions are like: \n",
      "[array([ 0.9805249 ,  0.01947507], dtype=float32), array([ 0.99372512,  0.00627485], dtype=float32), array([ 0.03518222,  0.96481776], dtype=float32), array([ 0.93461794,  0.06538212], dtype=float32), array([ 0.00934731,  0.99065268], dtype=float32), array([ 0.98951858,  0.01048141], dtype=float32), array([ 0.9609549 ,  0.03904507], dtype=float32), array([ 0.95169759,  0.0483024 ], dtype=float32), array([ 0.990026  ,  0.00997404], dtype=float32), array([ 0.60694057,  0.39305946], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print 'Some predictions are like: '\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Output the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csvfile = file('csvtest.csv', 'wb')\n",
    "writer = csv.writer(csvfile)\n",
    "writer.writerow(['id', 'realDonaldTrump', 'HillaryClinton'])\n",
    "data = []\n",
    "for i in range(test_length):\n",
    "    data.append((i, predictions[i][1], predictions[i][0]))\n",
    "\n",
    "writer.writerows(data)\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After TF-IDF, the input matrix is of size: (4743, 7656)\n"
     ]
    }
   ],
   "source": [
    "#define vectorizer parameters\n",
    "vectorizer = CountVectorizer(decode_error = 'ignore')\n",
    "transformer = TfidfTransformer(norm = 'l2', use_idf = True)\n",
    "\n",
    "# Apply TF-IDF to the training set\n",
    "# Use fit_transform() to learn the matrix model\n",
    "tfidf_matrix = transformer.fit_transform(vectorizer.fit_transform(train_corpus_tokenized))\n",
    "X_train = tfidf_matrix.toarray()\n",
    "print 'After TF-IDF, the input matrix is of size: ' + str(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After TF-IDF, the input matrix is of size: (1701, 7656)\n"
     ]
    }
   ],
   "source": [
    "# Apply TF-IDF to the testing set\n",
    "# Instead use transform() to apply the matrix model to the testing set\n",
    "X_test = transformer.transform(vectorizer.transform(test_corpus_tokenized))\n",
    "test_length = np.shape(X_test)[0]\n",
    "print 'After TF-IDF, the input matrix is of size: ' + str(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.89305199  0.10694801]\n",
      " [ 0.90574438  0.09425562]\n",
      " [ 0.23665331  0.76334669]\n",
      " ..., \n",
      " [ 0.32748839  0.67251161]\n",
      " [ 0.32560385  0.67439615]\n",
      " [ 0.45170627  0.54829373]]\n"
     ]
    }
   ],
   "source": [
    "# Apply logistic regression model to the training set\n",
    "LR_model = LogisticRegression()\n",
    "LR_model.fit(X_train, train_label)\n",
    "\n",
    "# Use the model to make prediction, output as probabilities\n",
    "predictions = LR_model.predict_proba(X_test)\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output the data just like before\n",
    "csvfile = file('csvtestLR.csv', 'wb')\n",
    "writer = csv.writer(csvfile)\n",
    "writer.writerow(['id', 'realDonaldTrump', 'HillaryClinton'])\n",
    "data = []\n",
    "for i in range(test_length):\n",
    "    data.append((i, predictions[i][1], predictions[i][0]))\n",
    "\n",
    "writer.writerows(data)\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reference and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this project we went to the following online resources for help: tensorflow word2vec tutorial: https://www.tensorflow.org/tutorials/word2vec, Andrej Karpathy blog: http://karpathy.github.io/2015/05/21/rnn-effectiveness/, Colah's blog - understanding LSTM: http://colah.github.io/posts/2015-08-Understanding-LSTMs/, r2rt blog: <a href=\"https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\">Recurrent Neural Networks in Tensorflow III - Variable Length Sequences</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tuned hyper parameters and found out that the parameters specified in this notebook produce the 'for-this-moment' best result, which is 0.244.\n",
    "<p>Lastly in order to compare the model with 'traditional' machine learning models and see who is better, we selected logistic regression as a representative. The final output seems also reasonalble and the Kaggle score is 0.327, just a little bit worse than our model implementing RNN. Maybe, if the dataset is larger, say more than 10,000 or even larger, RNN would be able to perform better than logistic regression more obviously.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
